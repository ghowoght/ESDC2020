#!/usr/bin/env python3
# #-*- coding: UTF-8 -*- 
import numpy as np
from scipy import signal
import matplotlib.animation as animation
import matplotlib.lines as line
import queue
import threading
import pyaudio
import matplotlib.pyplot as plt
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from PyQt5.QtWidgets import QApplication, QMainWindow, QVBoxLayout, QSizePolicy, QWidget
from PyQt5 import QtCore
import sys
import matplotlib
matplotlib.use("Qt5Agg")
'''å¯¼å…¥ä¾èµ–åº“'''
import struct as st
import time
from .VAD import VAD
from .SpeechRecognizer import SpeechRecognizer
from .SpeechSynthesis import speech_synthesis
import socket
import subprocess



speech_synthesis("ä½ å¥½ï¼Œæˆ‘æ˜¯å°äº”")

# CHUNK = 1024  # å®šä¹‰æ•°æ®æµå—
# WIDTH = 2
# CHANNELS = 2
# RATE = 16000 # sample 
# CHUNK = 512  # å®šä¹‰æ•°æ®æµå—
# WIDTH = 2
# CHANNELS = 2
# RATE = 16000 # sample 

#######################################################################
'''æ›´æ”¹éŸ³é¢‘æµæ ¼å¼'''
CHUNK = 500
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
RECORD_SECONDS = 3

'''å‚æ•°è®¾ç½®'''
STATUS_WAITING_WAKE_UP = 0
STATUS_WAITING_FOR_COMMAND = 1
STATUS_WAITING_FOR_CONFIRM = 2
STATUS_WORKING = 3
WAKE_UP_WORDS = "å°äº”å°äº”"
wake_up_words_list = ["å°äº”å°äº”", "æƒ³æˆ‘æƒ³æˆ‘", "ä¸‹åˆä¸‹åˆ", "å°æ­¦å°æ­¦", "ç¬‘æˆ‘ç¬‘æˆ‘", "æˆ‘æƒ³æˆ‘"]
hero_cmds = ["mapping_semantic", "navigation", "nav_semantic", "stop"]
aibox_cmds = ["mapping_semantic", "navigation", "nav_semantic", "stop", "return_origin"]

servers = ["æœå¯»ç›®æ ‡", "æ„å»ºè¯­ä¹‰åœ°å›¾", "æ„å»ºåœ°å›¾", "è¿”å›åŸç‚¹", "è¿”å›èµ·ç‚¹", "åœæ­¢é”®å›¾", 
                    "åœæ­¢æœå¯»", "åœæ­¢å»ºå›¾", "æŸ¥è¯¢æ—¶é—´", "åœæ­¢è§å›¾", "å¼€å§‹å¯¼èˆª", "åœæ­¢å¯¼èˆª"]
servers_dict = {"æœå¯»ç›®æ ‡" : "nav_semantic", 
                                "æ„å»ºè¯­ä¹‰åœ°å›¾" : "mapping_semantic", 
                                "æ„å»ºåœ°å›¾" : "mapping_semantic", 
                                "å¼€å§‹å¯¼èˆª" : "navigation", 
                                "è¿”å›åŸç‚¹" : "return_origin", 
                                "è¿”å›èµ·ç‚¹" : "return_origin", 
                                "åœæ­¢æœå¯»" : "stop", 
                                "åœæ­¢å¯¼èˆª" : "stop", 
                                "åœæ­¢å»ºå›¾" : "stop", 
                                "åœæ­¢é”®å›¾" : "stop", 
                                "åœæ­¢è§å›¾" : "stop", 
                                "æŸ¥è¯¢æ—¶é—´" : ""}
# å‘½ä»¤æŸ¥è¯¢å­—å…¸ AI-Boxç«¯
cmds = {"mapping_semantic":"roslaunch xrobot aibox_mapping_and_semantic.launch", # æ„å»ºè¯­ä¹‰åœ°å›¾
                "navigation":"roslaunch xrobot aibox.launch", # åœ¨åœ°å›¾ä¸­å®šç‚¹å¯¼èˆª
                "nav_semantic":"roslaunch xrobot aibox_nav_semantic.launch", # åœ¨åœ°å›¾ä¸­è¯­ä¹‰æ ‡æ³¨
                "return_origin":"python /home/uzei/myrobot/src/xrobot/scripts/ReturnOrigin.py",
                "stop":"python3 /home/uzei/myrobot/src/xrobot/setup/aibox_end.py"} # åœæ­¢

def send_cmd(cmd):
    ## å‘é€æŒ‡ä»¤ç»™æœºå™¨äºº
    host = '192.168.3.7' # ip
    port = 8000 # ç«¯å£å·
    address = (host, port)
    tcpClient = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # åˆ›å»ºä¸€ä¸ªå¥—æ¥å­—
    tcpClient.connect(address) # è¿æ¥æœºå™¨äºº

    tcpClient.send(bytes(cmd, encoding="utf-8")) # å‘é€æŒ‡ä»¤
    data = tcpClient.recv(1024) #æ¥æ”¶åé¦ˆå€¼
    if data.decode() == b"ok":
        pass
    tcpClient.close() # å…³é—­è¿æ¥

def excu_server(svr):
    if svr in hero_cmds:
        send_cmd(svr)
        time.sleep(5)
    if svr in aibox_cmds:
        p = subprocess.Popen(cmds[svr], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        # p = subprocess.run(cmds[svr], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        for line in iter(p.stdout.readline, b''):
            print(line.decode())
        p.stdout.close()
        p.wait()
        if svr == "stop":
            # p.wait()
            speech_synthesis("å·²åœæ­¢å®Œæ¯•ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡")

state = STATUS_WAITING_WAKE_UP
play_flag = False # åœ¨è¿›è¡Œè¯­éŸ³æ’­æŠ¥æ—¶ä¸è¿›è¡Œå…¶ä»–æ“ä½œï¼Œç›¸å½“äºä¸€æŠŠğŸ”“
last_index = 0
def si_thread(index, frame):
    ## è¯­éŸ³äº¤äº’çº¿ç¨‹
    global state, play_flag, last_index
    
    # è¯­éŸ³è¯†åˆ«
    sr = SpeechRecognizer(frame) 
    sr.recognize()
    # print(str(index) + " " + str(last_index) + " " + sr.ws.result)
    print(sr.ws.result)
    if play_flag == False and index > last_index:
        
        if state == STATUS_WAITING_WAKE_UP: # å¤„äºç­‰å¾…å”¤é†’çŠ¶æ€
            # if sr.ws.result == WAKE_UP_WORDS: # å”¤é†’è¯
            if sr.ws.result in wake_up_words_list:
                last_index = index
                state = STATUS_WAITING_FOR_COMMAND
                play_flag = True
                speech_synthesis("å—¯ï¼Œä½ å¥½ï¼Œè¯·é—®æ‚¨æœ‰ä»€ä¹ˆå©å’")
                play_flag = False
        elif state == STATUS_WAITING_FOR_COMMAND: # å¤„äºç­‰å¾…å‘½ä»¤çŠ¶æ€
            if sr.ws.result in servers:
                last_index = index
                state = STATUS_WORKING
                play_flag = True
                speech_synthesis("å¥½çš„ï¼Œæ­£åœ¨" + sr.ws.result)
                # excu_server(servers_dict[sr.ws.result])
                play_flag = False
                
                state = STATUS_WAITING_WAKE_UP
                excu_server(servers_dict[sr.ws.result])
        # print("[INFO][mat]",state)

#######################################################################

class MyMplCanvas(FigureCanvas):
    """FigureCanvasçš„æœ€ç»ˆçš„çˆ¶ç±»å…¶å®æ˜¯QWidgetã€‚"""

    def __init__(self, parent=None, width=5, height=4, dpi=100):

        # é…ç½®ä¸­æ–‡æ˜¾ç¤º
        # plt.rcParams['font.family'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
        # plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

        # æ­¤å¤„åˆå§‹åŒ–å­å›¾ä¸€å®šè¦åœ¨åˆå§‹åŒ–å‡½æ•°ä¹‹å‰
        self.fig = plt.figure()
        self.fig.patch.set_alpha(0)
        self.rt_ax = plt.subplot(111, xlim=(0, CHUNK*1), ylim=(-20000, 20000))
        plt.axis('off')

        FigureCanvas.__init__(self, self.fig)
        self.setParent(parent)

        '''å®šä¹‰FigureCanvasçš„å°ºå¯¸ç­–ç•¥ï¼Œè¿™éƒ¨åˆ†çš„æ„æ€æ˜¯è®¾ç½®FigureCanvasï¼Œä½¿ä¹‹å°½å¯èƒ½çš„å‘å¤–å¡«å……ç©ºé—´ã€‚'''
        FigureCanvas.setSizePolicy(self,
                                   QSizePolicy.Expanding,
                                   QSizePolicy.Expanding)
        FigureCanvas.updateGeometry(self)


class MatplotlibWidget(QWidget):
    def __init__(self, parent=None):
        super(MatplotlibWidget, self).__init__(parent)
        self.initUi()
        self.initariateV()

    def initUi(self):
        self.layout = QVBoxLayout(self)
        self.mpl = MyMplCanvas(self, width=15, height=15, dpi=100)
        self.layout.addWidget(self.mpl)

    # åˆå§‹åŒ–æˆå‘˜å˜é‡
    def initariateV(self):
        self.p = None
        self.q = queue.Queue()
        self.t = None
        self.ad_rdy_ev = None
        self.stream = None
        self.window = None
        self.ani = None
        self.rt_line = line.Line2D(xdata=[], ydata=[],linewidth=1,color='#FFB6C1')  # ç›´çº¿å¯¹è±¡

        self.rt_x_data = np.arange(0, CHUNK*1, 1)
        self.rt_data = np.full((CHUNK*1, ), 0)
        self.rt_line.set_xdata(self.rt_x_data)  # åˆå§‹åŒ–æ¨ªåæ ‡
        self.rt_line.set_ydata(self.rt_data)  # åˆå§‹åŒ–çºµåæ ‡

    # å¼€å§‹å½•åˆ¶è§¦å‘å‡½æ•°

    def startAudio(self, *args, **kwargs):
        # self.mpl.fig.suptitle('audio wave')
        self.ani = animation.FuncAnimation(
            self.mpl.fig, self.plot_update,
            init_func=self.plot_init,
            frames=1,
            interval=10,
            blit=True)
        # å…¶å®animationæ–¹æ ¼å¼æ³•çš„å®è´¨æ˜¯å¼€å¯äº†ä¸€ä¸ªçº¿ç¨‹æ›´æ–°å›¾åƒ

        # éº¦å…‹é£å¼€å§‹è·å–éŸ³é¢‘
        self.p = pyaudio.PyAudio()
        '''æ›´æ”¹éŸ³é¢‘æµæ ¼å¼'''
        # æ‰“å¼€æ•°æ®æµ
        # self.stream = self.p.open(
        #     format=pyaudio.paInt16,
        #     channels=CHANNELS,
        #     rate=RATE,
        #     input=True,
        #     output=False,
        #     frames_per_buffer=CHUNK,
        #     stream_callback=self.callback)
        self.stream = self.p.open(format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK,
                stream_callback=self.callback)
        self.stream.start_stream()

        # æ­£æ€åˆ†å¸ƒæ•°ç»„ï¼Œä¸éŸ³é¢‘æ•°æ®åšç›¸å…³è¿ç®—å¯ä¿è¯æ³¢å½¢å›¾ä¸¤ç«¯å›ºå®š
        '''å•é€šé“*1'''
        self.window = signal.hamming(CHUNK*1)

        # åˆå§‹åŒ–çº¿ç¨‹
        self.ad_rdy_ev = threading.Event()  # çº¿ç¨‹äº‹ä»¶å˜é‡
        self.t = threading.Thread(
            target=self.read_audio_thead,
            args=(self.q, self.stream, self.ad_rdy_ev))  # åœ¨çº¿ç¨‹tä¸­æ·»åŠ å‡½æ•°read_audio_thead
        self.t.start()  # çº¿ç¨‹å¼€å§‹è¿è¡Œ
        self.mpl.draw()

        # animationçš„æ›´æ–°å‡½æ•°
    def plot_update(self, i):
        self.rt_line.set_xdata(self.rt_x_data)
        self.rt_line.set_ydata(self.rt_data)
        return self.rt_line,

    # animationçš„åˆå§‹åŒ–å‡½æ•°
    def plot_init(self):
        self.mpl.rt_ax.add_line(self.rt_line)
        return self.rt_line,

    # pyaudioçš„å›è°ƒå‡½æ•°
    def callback(self, in_data, frame_count, time_info, status):
        global ad_rdy_ev
        self.q.put(in_data)
        return (None, pyaudio.paContinue)
    frames = []
    cnt = 0
    def read_audio_thead(self, q, stream, ad_rdy_ev):
        # # è·å–é˜Ÿåˆ—ä¸­çš„æ•°æ®
        # while stream.is_active():
        #     self.ad_rdy_ev.wait(timeout=0.05)  # çº¿ç¨‹äº‹ä»¶ï¼Œç­‰å¾…0.1s
        #     if not q.empty():
        #         data = q.get()
        #         while not q.empty():  # å°†å¤šä½™çš„æ•°æ®æ‰”æ‰ï¼Œä¸ç„¶é˜Ÿåˆ—ä¼šè¶Šæ¥è¶Šé•¿
        #             q.get()
        #         self.rt_data = np.frombuffer(data, np.dtype('<i2'))
        #         self.rt_data = self.rt_data * self.window   # è¿™æ ·åšçš„ç›®çš„æ˜¯å°†æ›²çº¿çš„ä¸¤ç«¯å›ºå®šï¼Œä»¥å…å‡ºç°æ›²çº¿æ•´ä½“å‘ç”Ÿæ³¢åŠ¨
        #     self.ad_rdy_ev.clear()
        while stream.is_active():
            # print("ok")
            if not q.empty():
                data = q.get()
                self.frames.append(data)
                self.cnt = self.cnt + 1

                fmt = "<" + str(CHUNK) + "h"
                self.rt_data = np.array(st.unpack(fmt, bytes(data)), dtype=float)
                self.rt_data = self.rt_data * self.window   # è¿™æ ·åšçš„ç›®çš„æ˜¯å°†æ›²çº¿çš„ä¸¤ç«¯å›ºå®šï¼Œä»¥å…å‡ºç°æ›²çº¿æ•´ä½“å‘ç”Ÿæ³¢åŠ¨
                if len(self.frames) > RECORD_SECONDS * RATE / CHUNK:
                    del self.frames[0]
                datas = b''
                frames_new = []
                for i in range(len(self.frames)):
                    datas = datas + self.frames[i]
                    if i % 10 == 0:
                        frames_new.append(self.frames[i])
                    else:
                        frames_new[len(frames_new) - 1] = frames_new[len(frames_new) - 1] + self.frames[i]
                if len(datas) == RECORD_SECONDS * RATE * 2 and self.cnt % 10 == 0:
                    # å­—èŠ‚æµè§£ç 
                    fmt = "<" + str(RECORD_SECONDS * RATE) + "h"
                    data_decode = np.array(st.unpack(fmt, bytes(datas)))
                    # print("-------------")
                    # è¯­éŸ³ç«¯ç‚¹æ£€æµ‹
                    isVoice, segments = VAD(data_decode, RATE)
                    # print(isVoice)
                    # æ£€æµ‹åˆ°å£°éŸ³ï¼Œå¼€å¯ä¸€ä¸ªè¯­éŸ³äº¤äº’çº¿ç¨‹
                    if isVoice:
                        # self.cnt = self.cnt + 1
                        if play_flag == False:
                            t = threading.Thread(target=si_thread, args=(self.cnt, frames_new))
                            t.start()


    def endAudio(self):
        # åœæ­¢è·å–éŸ³é¢‘ä¿¡æ¯
        self.stream.stop_stream()
        self.stream.close()
        self.p.terminate()
        # æ¸…ç©ºé˜Ÿåˆ—
        while not self.q.empty():
            self.q.get()
        # é‡æ–°åˆå§‹åŒ–å˜é‡
        self.initariateV()


if __name__ == '__main__':
    app = QApplication(sys.argv)
    ui = MatplotlibWidget()
    ui.startAudio()
    ui.show()
    sys.exit(app.exec_())
